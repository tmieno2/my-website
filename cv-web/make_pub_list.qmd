---
title: Create a reference list for the website and CV
---

```{r}
library(bib2df)
library(yaml)
library(dplyr)
library(data.table)
 library(scholar)
```

# Create a database of publications

## Get a list of publications from Google Scholar

```{r}
#--- set my google scholar id ---#
id <- "PxS7aU0AAAAJ"

pub_data <-
  scholar::get_publications(id) %>%
  data.table() %>%
  .[, category := "article"]
```

## Get author information (full name). This is for a bib file.

```{r}
#--- get the full name of all the authors by publication ---#
comp_authors <-
  lapply(
    pub_data$pubid,
    \(x) {
      data.table(
        pubid = x,
        comp_authors = get_complete_authors(id, x) %>%
          attr("names")
      )
    }
  ) %>%
  rbindlist()
```

Merge with `pub_data` and replace "," with "and" to make them proper bib entries.

```{r}
pub_data <- comp_authors[pub_data, on = .(pubid)]
pub_data[, comp_authors := gsub(",", " and ", comp_authors)]
```

## Get urls of the paper (this is for the web-version)

```{r}
pub_urls <-
  lapply(
    1:nrow(pub_data),
    function(x) get_publication_url(id, pub_data$pubid[x])
  )

pub_urls[sapply(pub_urls, function(x) length(x) == 0L)] <- NA

pub_data$url <- unlist(pub_urls)
```

## Select articles (ignore irrelevant articles)

```{r}
pr_journal_ls <-
  pub_data$journal %>%
  unique() %>%
  .[!stringr::str_detect(., "CSSA|Working|Chapman|HORIZON|Japanese|OSF|tmieno|SSRN|Illinois|Cornhusker|Abstract|SocArXiv|Proceeding|Unjournal|agriculture|AGU")] %>%
  .[. != ""]
```

# Create a bib file for peer-reviewed journal articles

```{r}
pub_data[journal %in% pr_journal_ls, ] %>%
  .[, author := NULL] %>%
  setnames("comp_authors", "author") %>%
  bib2df::df2bib(here::here("Website/cv-web/data/publications.bib"))
```

# Create yaml files for the web version

## Define functions

### Get Github repositories 

Define a function that gets the link to the Github repositories specified by the user in `repository_list`.

```{r}
attach_repo_link <- function(pub_data, repository_list) {
  keyword_ls <- repository_list$keyword
  repolink_ls <- repository_list$repo_link
  pub_data_copy <- copy(pub_data)

  for (i in 1:length(keyword_ls)) {
    pub_data_copy[grepl(keyword_ls[i], title), repo_link := repolink_ls[i]]
  }

  num_link <- pub_data_copy[, sum(!is.na(repo_link))]

  if (num_link < nrow(repository_list)) {
    print(paste0("After the match, you have only ", num_link, "repository links in the publication data after the match, while you provided ", nrow(repository_list), "links. Take a look at the keyword variable for typos and other forms of errors."))
  }

  return(pub_data_copy)
}
```

Define the list of GitHub repositories manually:

```{r}
repository_list <-
  tribble(
    ~keyword, ~repo_link,
    "Bias in economic", "https://github.com/tmieno2/GWR_value",
    "Aquifer depletion", "https://github.com/tmieno2/Drought-Production-Risk-Aquifer",
    "different trial designs", "https://github.com/tmieno2/Econ_Trial_Design_PA",
    "Causal forest approach", "https://github.com/tmieno2/CF_for_VRA"
  ) %>%
  data.table()
```

### Removing papers

Define a function that removes paper entries based on keywords.

```{r}
remove_papers <- function(data, keywords) {
  if (!is.null(keywords)) {
    drop_or_not <-
      lapply(
        1:length(keywords),
        \(x) {
          grepl(keywords[x], data[, title])
        }
      ) %>%
      do.call("+", .)

    return_data <-
      data.frame(data)[!drop_or_not, ] %>%
      data.table()
  } else {
    return_data <- data
  }

  return(return_data)
}
```

### Create yaml file

```{r}
make_pub_yml <- function(pub_data, journal_list, exclude = NULL) {
  pr_pub_data <-
    pub_data[journal %in% journal_list, ] %>%
    attach_repo_link(., repository_list) %>%
    remove_papers(., keywords = exclude) %>%
    .[, title_with_link := ifelse(
      !is.na(repo_link),
      paste0(title, " ([Paper](", url, ")", ", [GitHub Repository](", repo_link, "))"),
      paste0(title, " ([Paper](", url, "))")
    )] %>%
    .[, .(title_with_link, year, journal)] %>%
    .[, year := as.character(year)] %>%
    .[, title_with_link := paste0('"', title_with_link, '"')]

  pub_list_in_yaml <-
    lapply(
      1:nrow(pr_pub_data),
      function(x) {
        yaml::as.yaml(pr_pub_data[x, ])
      }
    ) %>%
    unlist() %>%
    #--- add - as an indication of the start of a paper ---#
    gsub("title_with_link", "- title_with_link", .) %>%
    #--- add two spaces in front ---#
    gsub("year", "  year", .) %>%
    #--- add two spaces in front ---#
    gsub("journal", "  journal", .) %>%
    gsub("url", "  url", .) %>%
    gsub("\'", "", .)

  return(pub_list_in_yaml)
}
```

## All the peer-reviewed journal articles 

This is for the "Publications" page.

```{r}
pub_list_in_yaml <- make_pub_yml(pub_data, pr_journal_ls)

writeLines(pub_list_in_yaml, "Website/publications/publications.yml")
```

## Topic-specific publication list

### Precision-Ag

```{r}
PA_journal_ls <-
  c(
    "Frontiers in Agronomy",
    "Computers and Electronics in Agriculture",
    "Agricultural Systems",
    "Precision Agriculture"
  )

PA_pub_list_in_yaml <- make_pub_yml(pub_data, PA_journal_ls)

writeLines(PA_pub_list_in_yaml, "Website/projects/precision-ag/publications.yml")
```

### Water management

```{r}
WM_journal_ls <-
  c(
    "Water Resources Research",
    "American Journal of Agricultural Economics",
    "Resource and Energy Economics",
    "Advances in Water Resources",
    "Land Economics",
    "Nature Water",
    "Agricultural Economics",
    "Environmental and Resource Economics"
  )

WM_pub_list_in_yaml <-
  make_pub_yml(
    pub_data = pub_data,
    journal_list = WM_journal_ls,
    exclude = c("Input use under")
  )

writeLines(WM_pub_list_in_yaml, "Website/projects/water-economics/publications.yml")
```
